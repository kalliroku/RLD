# 강화학습 던전 (RL Dungeon) - Game Design Document

## 1. 개요 (Overview)

### 1.1 컨셉
강화학습을 게임화한 교육적 엔터테인먼트. 플레이어는 AI 모험가를 훈련시켜 타인의 던전을 공략하거나, 자신만의 던전을 설계해 다른 모험가들을 격파한다.

### 1.2 핵심 가치
- 강화학습의 핵심 개념(보상, 정책, 탐험/착취)을 직관적으로 체험
- 던전 제작자 vs 모험가의 대립 구조를 통한 재미
- 코드를 몰라도 RL을 체험, 알면 더 깊이 커스터마이징

### 1.3 타겟 유저
- 강화학습에 관심있는 입문자/학생
- 게임으로 배우고 싶은 개발자
- 던전 메이커류 게임을 좋아하는 유저

---

## 2. 핵심 루프 (Core Loop)

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│   [던전 선택] → [모험가 파견] → [탐험 실행] → [결과 확인]   │
│        ↑                                           │        │
│        │         ← [AI 학습/정책 개선] ←          │        │
│        │                                           ↓        │
│        └──────────── [보상 획득] ←─────────────────┘        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 2.1 모험가 모드
1. 던전 리스트에서 도전할 던전 선택
2. 모험가(들)를 파티로 구성
3. 탐험 모드 선택:
   - **수동 모드**: 직접 조작하며 플레이 (데이터 수집용)
   - **자동 모드**: 학습된 AI가 자동 플레이
   - **학습 모드**: 반복 시뮬레이션으로 정책 개선
4. 결과에 따라 보상/경험치 획득

### 2.2 던전 마스터 모드
1. 그리드 월드 에디터로 던전 설계
2. 타일 배치: 벽, 함정, 몬스터, 보물 등
3. 던전 검증 (서버에서 RL로 클리어 가능 여부 확인)
4. 던전 공개 → 다른 유저의 도전 → 격파 시 명성 획득

---

## 3. 시스템 설계

### 3.1 던전 시스템 (Grid World)

#### 3.1.1 기본 구조
- **크기**: 최소 5x5 ~ 최대 20x20
- **시작점**: 모험가 스폰 위치 (S)
- **목표점**: 클리어 조건 (G)
- **시야**: 모험가 주변 N칸만 관측 가능 (부분 관측)

#### 3.1.2 타일 종류

| 타일 | 코드 | 효과 | 보상 |
|------|------|------|------|
| 빈 칸 | `.` | 이동 가능 | 0 |
| 벽 | `#` | 이동 불가 | -1 (시도 시) |
| 시작점 | `S` | 스폰 위치 | 0 |
| 목표점 | `G` | 클리어 | +100 |
| 함정 | `T` | HP 감소 | -10 |
| 독 늪 | `P` | 지속 피해 | -5/턴 |
| 회복 샘 | `H` | HP 회복 | +5 |
| 보물 | `$` | 보너스 점수 | +20 |
| 열쇠 | `K` | 문 해제 아이템 | +5 |
| 잠긴 문 | `D` | 열쇠 필요 | 0 |
| 몬스터 | `M` | 전투 발생 | 승리: +15, 패배: -30 |
| 보스 | `B` | 강력한 전투 | 승리: +50, 패배: -50 |
| 워프 | `W` | 다른 워프로 이동 | 0 |
| 일방통행 | `>v<^` | 해당 방향만 이동 | 0 |

#### 3.1.3 예시 던전 (7x7)
```
#######
#S....#
#.###.#
#...#.#
#.#.T.#
#.#...G
#######
```

### 3.2 모험가 시스템 (Agent)

#### 3.2.1 기본 속성
```python
class Adventurer:
    name: str           # 이름
    hp: int             # 체력 (기본 100)
    max_hp: int         # 최대 체력
    attack: int         # 공격력
    defense: int        # 방어력
    speed: int          # 행동 우선순위
    class_type: str     # 직업
    policy: Policy      # 학습된 정책
    experience: int     # 경험치
    level: int          # 레벨
```

#### 3.2.2 직업 (Class)

| 직업 | HP | ATK | DEF | SPD | 특수 능력 |
|------|-----|-----|-----|-----|----------|
| 전사 (Warrior) | 150 | 15 | 10 | 5 | 함정 피해 50% 감소 |
| 척후 (Scout) | 80 | 10 | 5 | 15 | 시야 +1, 함정 탐지 |
| 힐러 (Healer) | 100 | 5 | 5 | 10 | 매 턴 파티원 HP +2 |
| 마법사 (Mage) | 70 | 20 | 3 | 8 | 원거리 공격 가능 |
| 탱커 (Tank) | 200 | 8 | 15 | 3 | 파티원 피해 대신 받기 |

#### 3.2.3 행동 공간 (Action Space)
```python
# 기본 행동 (Discrete)
actions = {
    0: "UP",
    1: "DOWN",
    2: "LEFT",
    3: "RIGHT",
    4: "WAIT",      # 제자리 대기
    5: "INTERACT",  # 상호작용 (열쇠 줍기, 문 열기 등)
    6: "ATTACK",    # 전투 시 공격
    7: "SKILL",     # 스킬 사용
}
```

#### 3.2.4 관측 공간 (Observation Space)
```python
observation = {
    "visible_tiles": np.array,  # 시야 내 타일 정보 (NxN)
    "position": (x, y),         # 현재 위치
    "hp": int,                  # 현재 HP
    "inventory": list,          # 보유 아이템
    "party_status": list,       # 파티원 상태 (파티 모드)
}
```

### 3.3 강화학습 시스템

#### 3.3.1 MDP 정의
- **State**: 현재 관측 (위치, 시야 내 타일, HP, 인벤토리)
- **Action**: 이동, 대기, 상호작용, 전투 액션
- **Reward**: 타일별 즉시 보상 + 클리어/실패 보상
- **Transition**: 결정적 (함정 등 일부 확률적)
- **Discount (γ)**: 0.99

#### 3.3.2 보상 설계
```python
rewards = {
    "step": -0.1,           # 매 스텝 약간의 페널티 (빠른 클리어 유도)
    "goal_reached": +100,   # 목표 도달
    "death": -50,           # 사망
    "treasure": +20,        # 보물 획득
    "trap_hit": -10,        # 함정 피해
    "wall_bump": -1,        # 벽 충돌
    "monster_kill": +15,    # 몬스터 처치
    "healing": +5,          # 회복
    "explore_new": +1,      # 새 칸 탐험
}
```

#### 3.3.3 알고리즘 캐릭터 시스템 (구현 완료)

David Silver UCL RL 강의 기반으로 알고리즘을 캐릭터화. 각 캐릭터는 실제 다른 RL 알고리즘으로 학습하며, 던전에서 눈에 보이는 행동 차이를 보여줌.

**현재 구현된 캐릭터 (11종)**

| 캐릭터 | 알고리즘 | 강의 | 핵심 특성 | 설명 |
|--------|---------|------|----------|------|
| **Q군** | Q-Learning | L5 | Off-policy, 낙관적 | 좌표를 외워서 학습. 던전별 전문가. |
| **스카우트** | Local Q-Learning | L5 | 관찰형 전이학습 | 주변을 관찰해서 학습. 던전 간 경험 공유. |
| **사르사** | SARSA | L5 | On-policy, 신중함 | 실수에서 배우는 신중파. 안전한 길을 선호. |
| **몬테** | Monte Carlo | L4 | 에피소드 단위 학습 | 끝까지 가봐야 안다! 완주 후 복기하는 사색가. |
| **트레이서** | SARSA(λ) | L4 | 적격 흔적 | 발자취를 남기며 학습. 먼 과거의 선택도 평가. |
| **다이나** | Dyna-Q | L8 | 모델 기반 계획 | 상상력의 달인. 경험을 머릿속에서 반복 재생. |
| **그래디** | REINFORCE | L7 | 정책 경사법 | 직감형 탐험가. 확률로 판단, 다양한 경로 시도. |
| **크리틱** | Actor-Critic | L7 | 가치+정책 동시 학습 | 배우와 비평가를 겸비. 안정적이고 효율적. |
| **QV군** | QV-Learning | 논문 | Q+V 이중 학습 | Q와 V를 동시에 학습. 과대추정을 줄여 안정적. |
| **아클라** | ACLA | 논문 | 학습 오토마톤 | 확률을 직접 조작해 빠르게 정책을 바꿈. |
| **앙상블** | Ensemble (BM) | 논문 | 5알고리즘 합의 | 볼츠만 곱으로 최적 행동을 선택. |

> QV-Learning, ACLA, Ensemble은 Wiering & van Hasselt (2008) "Ensemble Algorithms in Reinforcement Learning" 논문 기반.

**쇼케이스 던전 (알고리즘별 차이 확인용)**

| 던전 | 대상 | 설계 의도 |
|------|------|----------|
| Lv.13 Cliff Walk | SARSA | Q군은 절벽 위 직행, 사르사는 안전 우회 |
| Lv.14 Long Hall | Monte Carlo | 장거리 무보상 구간, MC의 정확한 리턴 계산 |
| Lv.15 Multi Room | SARSA(λ) | 3개 방, eligibility trace 전파 효과 |
| Lv.16 Open Field | Dyna-Q | 넓은 공간, 모델 기반 계획으로 빠른 수렴 |
| Lv.17 Two Paths | REINFORCE | 대칭 경로, 확률적 정책 학습 |
| Lv.18 Dead End Labyrinth | TD계열 | 미로 + 타임아웃, TD 점진 학습 vs MC/PG 실패 |
| Lv.19 Narrow Bridge | SARSA | 양쪽 구덩이 다리, on-policy 안전 우회 학습 |
| Lv.20 Cliff Walking | SARSA | 클래식 절벽 걷기, 신중한 탐험으로 빠른 수렴 |
| Lv.21 Desert Crossing | Dyna-Q | 초대형 개방 공간(19×13), 모델 기반 10배 빠른 수렴 |
| Lv.22 Monster Arena | HP관리 | 몬스터+힐 필수 통과, useHpState HP 인식 학습 |
| Lv.23 The Mirage | Monte Carlo | 골드 줄줄이+구덩이 함정, MC 1회 사망으로 즉시 학습 |
| Lv.24 Paper Maze | 앙상블 | Wiering & van Hasselt (2008) 논문 원본 6×9 그리드 벤치마크 |
| Lv.25 Paper Maze+ | 앙상블 | 논문 미로에 함정·힐·몬스터·골드 추가, 서브 알고리즘별 위험 평가 차이 |

**골드 소비 메커니즘**: 골드 타일은 에피소드 내 최초 방문 시에만 보상을 부여하고 사라짐 (몬스터 처치와 동일 패턴). 반복 방문 시 보상 없음.

#### 3.3.4 알고리즘 NPC 가차 시스템 (미래 계획)

알고리즘을 캐릭터화하여 가차로 획득하는 시스템. 각 NPC는 고유한 성격과 특수 능력을 가짐.

**NPC 목록 (미래 확장)**

| NPC 이름 | 레어리티 | 알고리즘 | 성격/설정 | 특수 능력 |
|---------|---------|---------|----------|----------|
| **딥큐** | ⭐⭐ Rare | DQN | 천재 소년, 기억력 좋음 | 경험 재생 효율 +30% |
| **에이투씨** | ⭐⭐ Rare | A2C | 듀오 콤비, 비평가+행동가 | 안정적 학습 (분산 -20%) |
| **피피오** | ⭐⭐⭐ Epic | PPO | 균형잡힌 달인 | 정책 업데이트 안정성 +50% |
| **듀얼링** | ⭐⭐⭐ Epic | Dueling DQN | 이중인격, 가치 판단 | 상태 가치 추정 정확도 +25% |
| **삭** | ⭐⭐⭐⭐ Legendary | SAC | 자유로운 탐험가 | 탐험 보너스 +100%, 엔트로피 보상 |
| **레인보우** | ⭐⭐⭐⭐ Legendary | Rainbow DQN | 7가지 능력의 결정체 | 모든 스탯 +15% |

**가차 확률**
```
⭐ Common:      60%
⭐⭐ Rare:       30%
⭐⭐⭐ Epic:      8%
⭐⭐⭐⭐ Legendary: 2%
```

**천장 시스템**
- 50회 내 Epic 미획득 시 다음 가차 Epic 확정
- 100회 내 Legendary 미획득 시 다음 가차 Legendary 확정

#### 3.3.4 학습 인터페이스
```python
# 기본 패키지 사용
adventurer.set_algorithm("DQN")
adventurer.train(dungeon, episodes=1000)

# 커스텀 코드 (유저가 작성)
class MyPolicy(BasePolicy):
    def select_action(self, observation):
        # 유저 구현
        pass

    def learn(self, experience):
        # 유저 구현
        pass

adventurer.set_custom_policy(MyPolicy())
```

### 3.4 파티 시스템 (Multi-Agent)

#### 3.4.1 파티 구성
- 최소 1명 ~ 최대 4명
- 같은 직업 중복 가능
- 시너지 보너스 (예: 전사+힐러 = HP 회복량 +50%)

#### 3.4.2 Multi-Agent 접근법

**Option A: Centralized (중앙 집중)**
- 전체 파티를 하나의 에이전트로 취급
- 행동 공간: 각 파티원 행동의 조합
- 장점: 협동 전략 학습 용이
- 단점: 행동 공간 폭발

**Option B: Decentralized (분산)**
- 각 파티원이 독립 에이전트
- 공유 보상으로 협동 유도
- 장점: 확장성
- 단점: 협동 학습 어려움

**Option C: CTDE (Centralized Training, Decentralized Execution)**
- 학습 시 전체 정보 활용, 실행 시 각자 행동
- 현실적인 협동 가능
- 구현: QMIX, MAPPO 등

→ **MVP에서는 Option A (단순화)로 시작, 추후 Option C로 확장**

---

## 4. 던전 검증 시스템

### 4.1 검증 프로세스
```
[던전 제출] → [자동 검증] → [난이도 측정] → [공개]
```

### 4.2 검증 조건
1. **클리어 가능성**: 최적 정책으로 클리어 가능해야 함
2. **최소 경로**: 시작점에서 목표까지 경로 존재
3. **적정 난이도**: 너무 쉽거나 불가능하면 안 됨

### 4.3 난이도 측정 지표
```python
difficulty_score = {
    "path_length": int,         # 최단 경로 길이
    "trap_density": float,      # 함정 비율
    "monster_count": int,       # 몬스터 수
    "required_hp": int,         # 클리어에 필요한 최소 HP
    "optimal_reward": float,    # 최적 정책의 기대 보상
    "learning_episodes": int,   # 학습에 필요한 에피소드 수 (추정)
}

# 종합 난이도: 1~10 스케일
final_difficulty = calculate_difficulty(difficulty_score)
```

---

## 5. 경제 시스템

### 5.1 게임 내 재화

| 재화 | 획득 방법 | 사용처 |
|------|----------|--------|
| 골드 | 던전 클리어, 보물 | 알고리즘 패키지, 캐릭터 강화 |
| 명성 | 던전 제작자로서 승리 | 랭킹, 특수 아이템 해금 |
| 경험치 | 모험가 활동 | 레벨업, 능력치 증가 |
| 젬 (프리미엄) | 인앱 결제 | 프리미엄 패키지, 스킨 |

### 5.2 인앱 결제 아이템
- 프리미엄 RL 알고리즘 패키지 (PPO, SAC 등)
- 모험가 스킨
- 던전 테마 팩
- 학습 가속 (서버 GPU 사용)

---

## 6. UI/UX 설계

### 6.1 메인 화면
```
┌────────────────────────────────────────┐
│  🏰 강화학습 던전                    ⚙️ │
├────────────────────────────────────────┤
│                                        │
│   ┌──────────┐    ┌──────────┐        │
│   │  ⚔️      │    │  🏗️      │        │
│   │ 모험하기  │    │ 던전 만들기│        │
│   └──────────┘    └──────────┘        │
│                                        │
│   ┌──────────┐    ┌──────────┐        │
│   │  👥      │    │  📊      │        │
│   │ 모험가    │    │ 학습현황  │        │
│   └──────────┘    └──────────┘        │
│                                        │
├────────────────────────────────────────┤
│  💰 1,234  |  ⭐ 567  |  🎖️ 12       │
└────────────────────────────────────────┘
```

### 6.2 던전 플레이 화면
```
┌────────────────────────────────────────────────┐
│ 던전: 초보자의 시련 (Lv.3)         ⏸️  ❌      │
├────────────────────────────────────────────────┤
│                                                │
│     # # # # # # #                             │
│     # 😀 . . . . #     HP: ████████░░ 80/100  │
│     # . # # # . #     턴: 15                  │
│     # . . . # . #     획득 보상: +35          │
│     # . # . T . #                             │
│     # . # . . . G     [수동] [자동] [학습]    │
│     # # # # # # #                             │
│                                                │
├────────────────────────────────────────────────┤
│ 행동 로그:                                     │
│ > 오른쪽으로 이동 (+1 탐험 보상)              │
│ > 함정 발견! 척후가 경고                      │
├────────────────────────────────────────────────┤
│ [↑] [←][→] [↓]  |  [대기] [상호작용]        │
└────────────────────────────────────────────────┘
```

### 6.3 학습 현황 화면
```
┌────────────────────────────────────────────────┐
│ 학습 현황 - 전사 "용감한 김씨"                 │
├────────────────────────────────────────────────┤
│                                                │
│  알고리즘: DQN                                │
│  학습 에피소드: 2,341 / 5,000                 │
│                                                │
│  보상 그래프:                                  │
│  100┤           ╭──────                       │
│   50┤      ╭────╯                             │
│    0┤──────╯                                  │
│  -50┤                                         │
│     └────────────────────                     │
│      0    1000   2000   3000                  │
│                                                │
│  클리어율: 73% (최근 100 에피소드)           │
│  평균 보상: 67.3                              │
│  탐험률 (ε): 0.15                            │
│                                                │
│  [학습 계속] [학습 중단] [정책 저장]          │
└────────────────────────────────────────────────┘
```

---

## 7. 기술 스택 (권장)

### 7.1 MVP 단계
```
Frontend:
  - Vanilla JavaScript + HTML5 Canvas
  - 또는 React (확장성 고려)

Backend:
  - Python 3.10+
  - FastAPI (REST API)
  - SQLite (로컬 DB)

RL Engine:
  - Gymnasium (환경)
  - Stable-Baselines3 (알고리즘)
  - NumPy, PyTorch

Communication:
  - REST API (기본)
  - WebSocket (실시간 학습 상태)
```

### 7.2 확장 단계
```
- PostgreSQL (멀티유저 DB)
- Redis (세션, 캐싱)
- Docker (배포)
- AWS/GCP (서버 GPU 학습)
```

---

## 8. MVP 로드맵

### Phase 1: 핵심 시스템 (1-2주)
- [ ] 그리드 월드 환경 구현 (Gymnasium 호환)
- [ ] 기본 타일 (벽, 함정, 목표) 구현
- [ ] 단일 에이전트 (모험가) 구현
- [ ] Q-Learning 기본 학습

### Phase 2: 기본 UI (1주)
- [ ] 간단한 웹 UI (그리드 렌더링)
- [ ] 수동 조작 모드
- [ ] 학습 시작/중단 버튼
- [ ] 기본 통계 표시

### Phase 3: 던전 에디터 (1주)
- [ ] 타일 배치 에디터
- [ ] 던전 저장/불러오기
- [ ] 기본 검증 (경로 존재 여부)

### Phase 4: 확장 콘텐츠 (2주)
- [ ] 추가 타일 (몬스터, 보물 등)
- [ ] 다양한 직업
- [ ] DQN/PPO 알고리즘 추가
- [ ] 파티 시스템 (단순 버전)

### Phase 5: 온라인 기능 (이후)
- [ ] 유저 계정
- [ ] 던전 공유
- [ ] 랭킹 시스템

---

## 9. 리스크 및 고려사항

### 9.1 기술적 리스크
- **학습 시간**: 복잡한 던전은 학습에 오래 걸림 → 서버 학습 or 시간 제한
- **밸런스**: 알고리즘 간 성능 차이 → 던전 난이도로 조절
- **커스텀 코드 보안**: 유저 코드 실행 시 샌드박싱 필요

### 9.2 게임 디자인 리스크
- **학습 곡선**: RL 개념이 어려울 수 있음 → 튜토리얼 강화
- **재미 요소**: 학습 대기 시간이 지루할 수 있음 → 시각화, 미니게임

---

## 10. 용어 정리

| 용어 | 설명 |
|------|------|
| 정책 (Policy) | 상태에서 행동을 선택하는 규칙 |
| 보상 (Reward) | 행동의 결과로 받는 점수 |
| 에피소드 (Episode) | 한 번의 던전 도전 (시작~종료) |
| 탐험률 (Epsilon) | 랜덤 행동 확률 (탐험 vs 착취) |
| Q-값 (Q-Value) | 특정 상태에서 특정 행동의 기대 보상 |
| 경험 재생 (Experience Replay) | 과거 경험을 저장하고 재사용하는 기법 |

---

---

## 11. 던전 에디터 (구현 완료)

### 11.1 개요
GDD 2.2절의 "던전 마스터 모드" 핵심 기능. 브라우저 내에서 타일을 배치해 커스텀 던전을 만들고, AI를 훈련시켜 볼 수 있는 에디터.

### 11.2 UI 구조
- **탭 기반 모드 전환**: `[Play]` / `[Editor]` 탭으로 모드 전환
- **에디터 모드**: 게임 UI(HP, Gold, Steps, D-pad) 숨김, 에디터 컨트롤 패널 표시
- **공유 캔버스**: 기존 게임 캔버스를 에디터와 공유

### 11.3 기능 목록

| 기능 | 설명 |
|------|------|
| **타일 페인팅** | 9종 타일 팔레트에서 선택, 클릭/드래그로 배치 |
| **도구** | Brush / Eraser / Fill (BFS 기반 영역 채우기) |
| **START/GOAL 단일 강제** | 새 배치 시 기존 것 자동 제거 |
| **Undo/Redo** | Grid.toString() 스냅샷 스택 (최대 50) |
| **BFS 검증** | START/GOAL 존재 + 경로 존재 확인 |
| **저장/불러오기** | localStorage (`rld_custom_dungeons`) 기반 |
| **Play This Dungeon** | 검증 후 즉시 게임 모드로 전환하여 플레이/훈련 |
| **Quick Test** | 에디터 내에서 AI를 돌려보는 빠른 테스트 (모드 전환 없음) |
| **정책 오버레이** | Quick Test 결과의 Q-Value/Policy를 캔버스에 시각화 |
| **키보드 단축키** | 0-8 타일선택, Ctrl+Z/Y Undo/Redo, 우클릭 지우개 |
| **터치 지원** | 모바일에서 터치로 타일 배치 |
| **그리드 리사이즈** | 3x3 ~ 25x25, 기존 내용 최대 보존 |

### 11.4 커스텀 던전 규칙
- 골드 비용 0, 보상 0 (이코노미 영향 없음)
- 언락 제한 없음
- 드롭다운에 `[Custom] 이름` 형식으로 표시
- Q-table은 `rld_qtable_{char}_custom_{id}` 키로 저장

### 11.5 파일 구조
```
web/js/game/editor.js   # DungeonEditor 클래스 (신규)
web/js/main.js          # 탭 전환, playCustomDungeon(), 커스텀 던전 처리
web/index.html          # 모드 탭 + 에디터 컨트롤 패널
web/css/style.css       # 탭, 팔레트, 도구, 에디터 모드 스타일
```

---

## 12. Multi-Stage Dungeon System (구현 완료)

### 12.1 개요
여러 스테이지(층)를 묶어 하나의 "던전"으로 구성하는 시스템. 스테이지는 독립적으로 생성/관리되는 재사용 가능한 빌딩 블록이며, 던전은 스테이지들의 조합 레시피.

### 12.2 핵심 구조

```
Stage (스테이지) = 재사용 가능한 단위 Grid
    ↓ 참조
Dungeon (던전) = 스테이지 조합 레시피 (floor slot 배열)
    ↓ 실행
Run (실행) = 랜덤 변형이 확정된 구체적 1회 플레이
```

### 12.3 게임 규칙
- **HP 계승**: 이전 층 HP로 다음 층 시작
- **골드 보류**: 스테이지 중 수집한 골드는 임시 → 전체 클리어 시 확정, 사망 시 소실
- **답파율 추적**: 던전 단위 클리어 성공률

### 12.4 Variant System (랜덤 변형)
Floor 슬롯에 여러 스테이지 후보를 등록하면, 매 실행마다 weighted random으로 하나 선택.
- 좌표 암기형 알고리즘 (Q군 등): 변형 대응 약함 → 일반화의 한계 체감
- 관찰형 알고리즘 (스카우트): 변형 대응 강함 → 전이학습의 힘 체감

### 12.5 기술 구현: MultiStageGrid
Virtual Coordinate Stacking 방식으로 여러 Grid를 세로로 쌓아 기존 알고리즘에 투명하게 제공. 8개 알고리즘의 `runEpisode()`에 `tryAdvanceStage()` 3줄 추가로 연동.

### 12.6 에디터 UX
2-Tier Editor (Stage 탭 / Dungeon 탭):
- Stage 탭: 기존 에디터 (그리드 페인팅) → Stage Library에 저장
- Dungeon 탭: Dungeon Composer (Floor 슬롯 관리, 변형 설정, 규칙 설정)

> 상세 설계: `docs/MULTI_STAGE_DUNGEON.md` 참조

---

## 13. 앙상블 알고리즘 시스템 (구현 완료)

Wiering & van Hasselt (2008) "Ensemble Algorithms in Reinforcement Learning" 논문 기반 구현.

### 13.1 새 알고리즘 3종

| 알고리즘 | 캐릭터 | 파일 | 핵심 |
|---------|--------|------|------|
| QV-Learning | QV군 | `qv-learning.js` | Q(s,a) + V(s) 이중 테이블, 과대추정 감소 |
| ACLA | 아클라 | `acla.js` | Learning Automaton, 확률 벡터 직접 조작 |
| Ensemble (BM) | 앙상블 | `ensemble.js` | 5개 서브 알고리즘 Boltzmann Multiplication 결합 |

### 13.2 Boltzmann Multiplication

```
combined(a) = ∏_k p_k(a|s)   (모든 서브 알고리즘의 확률 곱)
정규화 후 행동 선택
```

- **Q-based** (QLearning, SARSA, QVLearning): `softmax(Q/temperature)` → 확률 변환
- **Prob-based** (ActorCritic, ACLA): 이미 확률 출력이므로 그대로 사용
- 서브 알고리즘 epsilon = 0 (탐험은 앙상블 레벨에서만)

### 13.3 설계 주의점: 거부권(Veto) 효과

Boltzmann Multiplication은 확률을 **곱**하므로, 하나의 서브 알고리즘이 특정 행동에 0%를 출력하면 전체 앙상블도 0%가 됨. 이는 구덩이(P, 즉사) 같은 극단적 위험 요소에서 Actor-Critic이 학습 실패 시 발생. 해결: 즉사 대신 HP 데미지(Monster) 사용으로 모든 서브 알고리즘이 회피 학습 가능하도록 설계.

### 13.4 Paper Maze 벤치마크

논문의 6×9 그리드를 Lv.24(원본), Lv.25(확장판)으로 재현. 모든 알고리즘 100% 수렴 확인.

---

*Document Version: 0.6*
*Last Updated: 2026-02-19*
*Author: RLD Team*
