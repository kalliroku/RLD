# 강화학습 던전 (RL Dungeon) - Game Design Document

## 1. 개요 (Overview)

### 1.1 컨셉
강화학습을 게임화한 교육적 엔터테인먼트. 플레이어는 AI 모험가를 훈련시켜 타인의 던전을 공략하거나, 자신만의 던전을 설계해 다른 모험가들을 격파한다.

### 1.2 핵심 가치
- 강화학습의 핵심 개념(보상, 정책, 탐험/착취)을 직관적으로 체험
- 던전 제작자 vs 모험가의 대립 구조를 통한 재미
- 코드를 몰라도 RL을 체험, 알면 더 깊이 커스터마이징

### 1.3 타겟 유저
- 강화학습에 관심있는 입문자/학생
- 게임으로 배우고 싶은 개발자
- 던전 메이커류 게임을 좋아하는 유저

---

## 2. 핵심 루프 (Core Loop)

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│   [던전 선택] → [모험가 파견] → [탐험 실행] → [결과 확인]   │
│        ↑                                           │        │
│        │         ← [AI 학습/정책 개선] ←          │        │
│        │                                           ↓        │
│        └──────────── [보상 획득] ←─────────────────┘        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 2.1 모험가 모드
1. 던전 리스트에서 도전할 던전 선택
2. 모험가(들)를 파티로 구성
3. 탐험 모드 선택:
   - **수동 모드**: 직접 조작하며 플레이 (데이터 수집용)
   - **자동 모드**: 학습된 AI가 자동 플레이
   - **학습 모드**: 반복 시뮬레이션으로 정책 개선
4. 결과에 따라 보상/경험치 획득

### 2.2 던전 마스터 모드
1. 그리드 월드 에디터로 던전 설계
2. 타일 배치: 벽, 함정, 몬스터, 보물 등
3. 던전 검증 (서버에서 RL로 클리어 가능 여부 확인)
4. 던전 공개 → 다른 유저의 도전 → 격파 시 명성 획득

---

## 3. 시스템 설계

### 3.1 던전 시스템 (Grid World)

#### 3.1.1 기본 구조
- **크기**: 최소 5x5 ~ 최대 20x20
- **시작점**: 모험가 스폰 위치 (S)
- **목표점**: 클리어 조건 (G)
- **시야**: 모험가 주변 N칸만 관측 가능 (부분 관측)

#### 3.1.2 타일 종류

| 타일 | 코드 | 효과 | 보상 |
|------|------|------|------|
| 빈 칸 | `.` | 이동 가능 | 0 |
| 벽 | `#` | 이동 불가 | -1 (시도 시) |
| 시작점 | `S` | 스폰 위치 | 0 |
| 목표점 | `G` | 클리어 | +100 |
| 함정 | `T` | HP 감소 | -10 |
| 독 늪 | `P` | 지속 피해 | -5/턴 |
| 회복 샘 | `H` | HP 회복 | +5 |
| 보물 | `$` | 보너스 점수 | +20 |
| 열쇠 | `K` | 문 해제 아이템 | +5 |
| 잠긴 문 | `D` | 열쇠 필요 | 0 |
| 몬스터 | `M` | 전투 발생 | 승리: +15, 패배: -30 |
| 보스 | `B` | 강력한 전투 | 승리: +50, 패배: -50 |
| 워프 | `W` | 다른 워프로 이동 | 0 |
| 일방통행 | `>v<^` | 해당 방향만 이동 | 0 |

#### 3.1.3 예시 던전 (7x7)
```
#######
#S....#
#.###.#
#...#.#
#.#.T.#
#.#...G
#######
```

### 3.2 모험가 시스템 (Agent)

#### 3.2.1 기본 속성
```python
class Adventurer:
    name: str           # 이름
    hp: int             # 체력 (기본 100)
    max_hp: int         # 최대 체력
    attack: int         # 공격력
    defense: int        # 방어력
    speed: int          # 행동 우선순위
    class_type: str     # 직업
    policy: Policy      # 학습된 정책
    experience: int     # 경험치
    level: int          # 레벨
```

#### 3.2.2 직업 (Class)

| 직업 | HP | ATK | DEF | SPD | 특수 능력 |
|------|-----|-----|-----|-----|----------|
| 전사 (Warrior) | 150 | 15 | 10 | 5 | 함정 피해 50% 감소 |
| 척후 (Scout) | 80 | 10 | 5 | 15 | 시야 +1, 함정 탐지 |
| 힐러 (Healer) | 100 | 5 | 5 | 10 | 매 턴 파티원 HP +2 |
| 마법사 (Mage) | 70 | 20 | 3 | 8 | 원거리 공격 가능 |
| 탱커 (Tank) | 200 | 8 | 15 | 3 | 파티원 피해 대신 받기 |

#### 3.2.3 행동 공간 (Action Space)
```python
# 기본 행동 (Discrete)
actions = {
    0: "UP",
    1: "DOWN",
    2: "LEFT",
    3: "RIGHT",
    4: "WAIT",      # 제자리 대기
    5: "INTERACT",  # 상호작용 (열쇠 줍기, 문 열기 등)
    6: "ATTACK",    # 전투 시 공격
    7: "SKILL",     # 스킬 사용
}
```

#### 3.2.4 관측 공간 (Observation Space)
```python
observation = {
    "visible_tiles": np.array,  # 시야 내 타일 정보 (NxN)
    "position": (x, y),         # 현재 위치
    "hp": int,                  # 현재 HP
    "inventory": list,          # 보유 아이템
    "party_status": list,       # 파티원 상태 (파티 모드)
}
```

### 3.3 강화학습 시스템

#### 3.3.1 MDP 정의
- **State**: 현재 관측 (위치, 시야 내 타일, HP, 인벤토리)
- **Action**: 이동, 대기, 상호작용, 전투 액션
- **Reward**: 타일별 즉시 보상 + 클리어/실패 보상
- **Transition**: 결정적 (함정 등 일부 확률적)
- **Discount (γ)**: 0.99

#### 3.3.2 보상 설계
```python
rewards = {
    "step": -0.1,           # 매 스텝 약간의 페널티 (빠른 클리어 유도)
    "goal_reached": +100,   # 목표 도달
    "death": -50,           # 사망
    "treasure": +20,        # 보물 획득
    "trap_hit": -10,        # 함정 피해
    "wall_bump": -1,        # 벽 충돌
    "monster_kill": +15,    # 몬스터 처치
    "healing": +5,          # 회복
    "explore_new": +1,      # 새 칸 탐험
}
```

#### 3.3.3 알고리즘 NPC 가차 시스템

알고리즘을 캐릭터화하여 가차로 획득하는 시스템. 각 NPC는 고유한 성격과 특수 능력을 가짐.

**NPC 목록**

| NPC 이름 | 레어리티 | 알고리즘 | 성격/설정 | 특수 능력 |
|---------|---------|---------|----------|----------|
| **Q군** | ⭐ Common | Q-Learning | 성실한 신입, 기본기 충실 | 학습 속도 +20% |
| **사르사** | ⭐ Common | SARSA | 신중한 성격, 안전 제일 | 함정 피해 -10% |
| **딥큐** | ⭐⭐ Rare | DQN | 천재 소년, 기억력 좋음 | 경험 재생 효율 +30% |
| **에이투씨** | ⭐⭐ Rare | A2C | 듀오 콤비, 비평가+행동가 | 안정적 학습 (분산 -20%) |
| **피피오** | ⭐⭐⭐ Epic | PPO | 균형잡힌 달인 | 정책 업데이트 안정성 +50% |
| **듀얼링** | ⭐⭐⭐ Epic | Dueling DQN | 이중인격, 가치 판단 | 상태 가치 추정 정확도 +25% |
| **삭** | ⭐⭐⭐⭐ Legendary | SAC | 자유로운 탐험가 | 탐험 보너스 +100%, 엔트로피 보상 |
| **레인보우** | ⭐⭐⭐⭐ Legendary | Rainbow DQN | 7가지 능력의 결정체 | 모든 스탯 +15% |

**가차 확률**
```
⭐ Common:      60%
⭐⭐ Rare:       30%
⭐⭐⭐ Epic:      8%
⭐⭐⭐⭐ Legendary: 2%
```

**천장 시스템**
- 50회 내 Epic 미획득 시 다음 가차 Epic 확정
- 100회 내 Legendary 미획득 시 다음 가차 Legendary 확정

**NPC 강화**
- 같은 NPC를 중복 획득하면 "돌파" 가능
- 최대 6돌파 (능력치 +60%)

**가차 재화**
- **소환석**: 던전 클리어 보상, 일일 퀘스트
- **프리미엄 소환석**: 인앱 결제

#### 3.3.4 학습 인터페이스
```python
# 기본 패키지 사용
adventurer.set_algorithm("DQN")
adventurer.train(dungeon, episodes=1000)

# 커스텀 코드 (유저가 작성)
class MyPolicy(BasePolicy):
    def select_action(self, observation):
        # 유저 구현
        pass

    def learn(self, experience):
        # 유저 구현
        pass

adventurer.set_custom_policy(MyPolicy())
```

### 3.4 파티 시스템 (Multi-Agent)

#### 3.4.1 파티 구성
- 최소 1명 ~ 최대 4명
- 같은 직업 중복 가능
- 시너지 보너스 (예: 전사+힐러 = HP 회복량 +50%)

#### 3.4.2 Multi-Agent 접근법

**Option A: Centralized (중앙 집중)**
- 전체 파티를 하나의 에이전트로 취급
- 행동 공간: 각 파티원 행동의 조합
- 장점: 협동 전략 학습 용이
- 단점: 행동 공간 폭발

**Option B: Decentralized (분산)**
- 각 파티원이 독립 에이전트
- 공유 보상으로 협동 유도
- 장점: 확장성
- 단점: 협동 학습 어려움

**Option C: CTDE (Centralized Training, Decentralized Execution)**
- 학습 시 전체 정보 활용, 실행 시 각자 행동
- 현실적인 협동 가능
- 구현: QMIX, MAPPO 등

→ **MVP에서는 Option A (단순화)로 시작, 추후 Option C로 확장**

---

## 4. 던전 검증 시스템

### 4.1 검증 프로세스
```
[던전 제출] → [자동 검증] → [난이도 측정] → [공개]
```

### 4.2 검증 조건
1. **클리어 가능성**: 최적 정책으로 클리어 가능해야 함
2. **최소 경로**: 시작점에서 목표까지 경로 존재
3. **적정 난이도**: 너무 쉽거나 불가능하면 안 됨

### 4.3 난이도 측정 지표
```python
difficulty_score = {
    "path_length": int,         # 최단 경로 길이
    "trap_density": float,      # 함정 비율
    "monster_count": int,       # 몬스터 수
    "required_hp": int,         # 클리어에 필요한 최소 HP
    "optimal_reward": float,    # 최적 정책의 기대 보상
    "learning_episodes": int,   # 학습에 필요한 에피소드 수 (추정)
}

# 종합 난이도: 1~10 스케일
final_difficulty = calculate_difficulty(difficulty_score)
```

---

## 5. 경제 시스템

### 5.1 게임 내 재화

| 재화 | 획득 방법 | 사용처 |
|------|----------|--------|
| 골드 | 던전 클리어, 보물 | 알고리즘 패키지, 캐릭터 강화 |
| 명성 | 던전 제작자로서 승리 | 랭킹, 특수 아이템 해금 |
| 경험치 | 모험가 활동 | 레벨업, 능력치 증가 |
| 젬 (프리미엄) | 인앱 결제 | 프리미엄 패키지, 스킨 |

### 5.2 인앱 결제 아이템
- 프리미엄 RL 알고리즘 패키지 (PPO, SAC 등)
- 모험가 스킨
- 던전 테마 팩
- 학습 가속 (서버 GPU 사용)

---

## 6. UI/UX 설계

### 6.1 메인 화면
```
┌────────────────────────────────────────┐
│  🏰 강화학습 던전                    ⚙️ │
├────────────────────────────────────────┤
│                                        │
│   ┌──────────┐    ┌──────────┐        │
│   │  ⚔️      │    │  🏗️      │        │
│   │ 모험하기  │    │ 던전 만들기│        │
│   └──────────┘    └──────────┘        │
│                                        │
│   ┌──────────┐    ┌──────────┐        │
│   │  👥      │    │  📊      │        │
│   │ 모험가    │    │ 학습현황  │        │
│   └──────────┘    └──────────┘        │
│                                        │
├────────────────────────────────────────┤
│  💰 1,234  |  ⭐ 567  |  🎖️ 12       │
└────────────────────────────────────────┘
```

### 6.2 던전 플레이 화면
```
┌────────────────────────────────────────────────┐
│ 던전: 초보자의 시련 (Lv.3)         ⏸️  ❌      │
├────────────────────────────────────────────────┤
│                                                │
│     # # # # # # #                             │
│     # 😀 . . . . #     HP: ████████░░ 80/100  │
│     # . # # # . #     턴: 15                  │
│     # . . . # . #     획득 보상: +35          │
│     # . # . T . #                             │
│     # . # . . . G     [수동] [자동] [학습]    │
│     # # # # # # #                             │
│                                                │
├────────────────────────────────────────────────┤
│ 행동 로그:                                     │
│ > 오른쪽으로 이동 (+1 탐험 보상)              │
│ > 함정 발견! 척후가 경고                      │
├────────────────────────────────────────────────┤
│ [↑] [←][→] [↓]  |  [대기] [상호작용]        │
└────────────────────────────────────────────────┘
```

### 6.3 학습 현황 화면
```
┌────────────────────────────────────────────────┐
│ 학습 현황 - 전사 "용감한 김씨"                 │
├────────────────────────────────────────────────┤
│                                                │
│  알고리즘: DQN                                │
│  학습 에피소드: 2,341 / 5,000                 │
│                                                │
│  보상 그래프:                                  │
│  100┤           ╭──────                       │
│   50┤      ╭────╯                             │
│    0┤──────╯                                  │
│  -50┤                                         │
│     └────────────────────                     │
│      0    1000   2000   3000                  │
│                                                │
│  클리어율: 73% (최근 100 에피소드)           │
│  평균 보상: 67.3                              │
│  탐험률 (ε): 0.15                            │
│                                                │
│  [학습 계속] [학습 중단] [정책 저장]          │
└────────────────────────────────────────────────┘
```

---

## 7. 기술 스택 (권장)

### 7.1 MVP 단계
```
Frontend:
  - Vanilla JavaScript + HTML5 Canvas
  - 또는 React (확장성 고려)

Backend:
  - Python 3.10+
  - FastAPI (REST API)
  - SQLite (로컬 DB)

RL Engine:
  - Gymnasium (환경)
  - Stable-Baselines3 (알고리즘)
  - NumPy, PyTorch

Communication:
  - REST API (기본)
  - WebSocket (실시간 학습 상태)
```

### 7.2 확장 단계
```
- PostgreSQL (멀티유저 DB)
- Redis (세션, 캐싱)
- Docker (배포)
- AWS/GCP (서버 GPU 학습)
```

---

## 8. MVP 로드맵

### Phase 1: 핵심 시스템 (1-2주)
- [ ] 그리드 월드 환경 구현 (Gymnasium 호환)
- [ ] 기본 타일 (벽, 함정, 목표) 구현
- [ ] 단일 에이전트 (모험가) 구현
- [ ] Q-Learning 기본 학습

### Phase 2: 기본 UI (1주)
- [ ] 간단한 웹 UI (그리드 렌더링)
- [ ] 수동 조작 모드
- [ ] 학습 시작/중단 버튼
- [ ] 기본 통계 표시

### Phase 3: 던전 에디터 (1주)
- [ ] 타일 배치 에디터
- [ ] 던전 저장/불러오기
- [ ] 기본 검증 (경로 존재 여부)

### Phase 4: 확장 콘텐츠 (2주)
- [ ] 추가 타일 (몬스터, 보물 등)
- [ ] 다양한 직업
- [ ] DQN/PPO 알고리즘 추가
- [ ] 파티 시스템 (단순 버전)

### Phase 5: 온라인 기능 (이후)
- [ ] 유저 계정
- [ ] 던전 공유
- [ ] 랭킹 시스템

---

## 9. 리스크 및 고려사항

### 9.1 기술적 리스크
- **학습 시간**: 복잡한 던전은 학습에 오래 걸림 → 서버 학습 or 시간 제한
- **밸런스**: 알고리즘 간 성능 차이 → 던전 난이도로 조절
- **커스텀 코드 보안**: 유저 코드 실행 시 샌드박싱 필요

### 9.2 게임 디자인 리스크
- **학습 곡선**: RL 개념이 어려울 수 있음 → 튜토리얼 강화
- **재미 요소**: 학습 대기 시간이 지루할 수 있음 → 시각화, 미니게임

---

## 10. 용어 정리

| 용어 | 설명 |
|------|------|
| 정책 (Policy) | 상태에서 행동을 선택하는 규칙 |
| 보상 (Reward) | 행동의 결과로 받는 점수 |
| 에피소드 (Episode) | 한 번의 던전 도전 (시작~종료) |
| 탐험률 (Epsilon) | 랜덤 행동 확률 (탐험 vs 착취) |
| Q-값 (Q-Value) | 특정 상태에서 특정 행동의 기대 보상 |
| 경험 재생 (Experience Replay) | 과거 경험을 저장하고 재사용하는 기법 |

---

*Document Version: 0.2*
*Last Updated: 2026-02-10*
*Author: RLD Team*
