# RL Dungeon (ê°•í™”í•™ìŠµ ë˜ì „)

ê°•í™”í•™ìŠµì„ ê²Œì„í™”í•œ êµìœ¡ì  ì—”í„°í…Œì¸ë¨¼íŠ¸ í”„ë¡œì íŠ¸.
AI ëª¨í—˜ê°€ë¥¼ í›ˆë ¨ì‹œì¼œ ë˜ì „ì„ ê³µëµí•˜ê±°ë‚˜, ìì‹ ë§Œì˜ ë˜ì „ì„ ì„¤ê³„í•´ ë‹¤ë¥¸ ëª¨í—˜ê°€ë“¤ì„ ê²©íŒŒí•˜ì„¸ìš”.

## í•µì‹¬ ì»¨ì…‰

- **ëª¨í—˜ê°€ ëª¨ë“œ**: AI ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµì‹œì¼œ ë˜ì „ í´ë¦¬ì–´
- **ë˜ì „ ë§ˆìŠ¤í„° ëª¨ë“œ**: ë¸Œë¼ìš°ì € ë‚´ ë˜ì „ ì—ë””í„°ë¡œ ì»¤ìŠ¤í…€ ë˜ì „ ì œì‘ + AI í›ˆë ¨
- **NPC ê°€ì°¨ ì‹œìŠ¤í…œ**: ì•Œê³ ë¦¬ì¦˜ì„ ìºë¦­í„°í™” (Qêµ°, í”¼í”¼ì˜¤, ì‚­ ë“±) (ì˜ˆì •)

## í˜„ì¬ ì§„í–‰ ìƒí™©

| Phase | ë‚´ìš© | ìƒíƒœ |
|-------|------|------|
| Phase 0 | í”„ë¡œì íŠ¸ ì…‹ì—… | âœ… ì™„ë£Œ |
| Phase 1 | ê·¸ë¦¬ë“œ ì›”ë“œ (íƒ€ì¼, ë˜ì „, ë Œë”ë§) | âœ… ì™„ë£Œ |
| Phase 2 | ì—ì´ì „íŠ¸ (ì´ë™, HP, ë³´ìƒ) | âœ… ì™„ë£Œ |
| Phase 3 | ê²Œì„ ë¡œì§ (Gymnasium í™˜ê²½) | âœ… ì™„ë£Œ |
| Phase 4 | Q-Learning | âœ… ì™„ë£Œ |
| Phase 5 | ì›¹ UI | âœ… ì™„ë£Œ |
| Phase 6 | ê²Œì„ í™•ì¥ (ì´ì½”ë…¸ë¯¸, ì „ì¥ì˜ ì•ˆê°œ) | âœ… ì™„ë£Œ |
| Phase 7 | ë˜ì „ ì–¸ë½, ëª¬ìŠ¤í„°, LfD | âœ… ì™„ë£Œ |
| Phase 8 | Q-Table ì €ì¥, ëª¨ë°”ì¼ í„°ì¹˜, AI í•™ìŠµ ì‹œê°í™” | âœ… ì™„ë£Œ |
| Phase 9 | ì‡¼ì¼€ì´ìŠ¤ ìŠ¤í…Œì´ì§€ í™•ì¥ + ê³¨ë“œ ì†Œë¹„ | âœ… ì™„ë£Œ |
| Phase 10 | ë˜ì „ ì—ë””í„° (ë¸Œë¼ìš°ì € ë‚´ ì œì‘ ë„êµ¬) | âœ… ì™„ë£Œ |
| Phase 11 | ë©€í‹°ìŠ¤í…Œì´ì§€ ë˜ì „ | âœ… ì™„ë£Œ |
| Phase 12 | Wiering & van Hasselt ì•™ìƒë¸” (QV, ACLA, Ensemble) | âœ… ì™„ë£Œ |
| Phase 13 | DQN ì‹¤í—˜ (Vanilla JS MLP) â€” ì½”ë“œ ì™„ì„±, UI ë¯¸ë“±ë¡ | ğŸ”¬ ì‹¤í—˜ |
| Phase 14 | ë…¼ë¬¸ ê¸°ë°˜ ê²€í†  + Expected SARSA, Double Q-Learning, í™•ë¥ ì  ë˜ì „, 25Ã—25 ë¯¸ë¡œ | âœ… ì™„ë£Œ |
| Phase 15 | n-step Tree Backup, Prioritized Sweeping, BSP+CA ì ˆì°¨ì  ìƒì„±, 50Ã—50 ë˜ì „ | âœ… ì™„ë£Œ |
| Phase 16 | NPC ê°€ì°¨ ì‹œìŠ¤í…œ | â³ ì˜ˆì • |

## ì„¤ì¹˜

```bash
# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python -m venv venv
venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

## ì‹¤í–‰

### ì§ì ‘ í”Œë ˆì´
```bash
python play_game.py
```
- ë°©í–¥í‚¤: ì´ë™
- R: ë¦¬ì…‹
- ESC: ì¢…ë£Œ

### AI í•™ìŠµ (Q-Learning)
```bash
python train_agent.py [ë˜ì „íŒŒì¼] [ì—í”¼ì†Œë“œìˆ˜]

# ì˜ˆì‹œ
python train_agent.py assets/dungeons/level_01_easy.txt 500
```

### ë˜ì „ ë·°ì–´
```bash
python run_viewer.py [ë˜ì „íŒŒì¼]
```

### Gymnasium í™˜ê²½ ë°ëª¨
```bash
python run_gym_env.py
```

### ì›¹ ë²„ì „ (ë¸Œë¼ìš°ì €)
```bash
cd web
python -m http.server 8080
# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8080/ ì ‘ì†
```
- ë°©í–¥í‚¤/WASD: ì´ë™ (ëª¨ë°”ì¼: ìŠ¤ì™€ì´í”„ ë˜ëŠ” D-pad)
- AI Training: ì‹œê°ì  í•™ìŠµ (1x/2x/3x ì†ë„) ë˜ëŠ” Instant ëª¨ë“œ
- Until Success: 95% ì„±ê³µë¥  ë„ë‹¬ ì‹œ ìë™ ì¢…ë£Œ
- Continuous: Stop ë²„íŠ¼ìœ¼ë¡œ ìˆ˜ë™ ì •ì§€ (ìµœëŒ€ 10,000 ì—í”¼ì†Œë“œ)
- Q-Table ìë™ ì €ì¥/ë³µì› (ìƒˆë¡œê³ ì¹¨í•´ë„ í•™ìŠµ ë°ì´í„° ìœ ì§€)
- Show Q-Values/Policy: í•™ìŠµ ì‹œê°í™”
- Fog of War: ì „ì¥ì˜ ì•ˆê°œ í† ê¸€

## ìƒ˜í”Œ ë˜ì „

```
assets/dungeons/
â”œâ”€â”€ level_01_easy.txt   # 5x5 ê¸°ë³¸
â”œâ”€â”€ level_02_trap.txt   # 7x7 í•¨ì •+íšŒë³µ
â””â”€â”€ level_03_maze.txt   # 9x9 ë¯¸ë¡œ
```

### íƒ€ì¼ ì¢…ë¥˜
| ë¬¸ì | íƒ€ì¼ | íš¨ê³¼ |
|-----|------|------|
| `.` | ë¹ˆ ì¹¸ | ì´ë™ ê°€ëŠ¥ |
| `#` | ë²½ | ì´ë™ ë¶ˆê°€ |
| `S` | ì‹œì‘ì  | ìŠ¤í° ìœ„ì¹˜ |
| `G` | ëª©í‘œ | í´ë¦¬ì–´ (+100) |
| `T` | í•¨ì • | HP -10 |
| `H` | íšŒë³µ | HP +10 |
| `P` | êµ¬ë©ì´ | ì¦‰ì‚¬ |
| `$` | ê³¨ë“œ | +10 ë³´ìƒ |
| `M` | ëª¬ìŠ¤í„° | HP -30, ì²˜ì¹˜ ì‹œ +5G |

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
RLD/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/           # íƒ€ì¼, ê·¸ë¦¬ë“œ
â”‚   â”œâ”€â”€ agents/         # ì—ì´ì „íŠ¸ (ëª¨í—˜ê°€)
â”‚   â”œâ”€â”€ algorithms/     # RL ì•Œê³ ë¦¬ì¦˜ (Q-Learning)
â”‚   â””â”€â”€ ui/             # Pygame ë Œë”ëŸ¬
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ index.html      # ì›¹ UI
â”‚   â”œâ”€â”€ css/style.css   # ìŠ¤íƒ€ì¼
â”‚   â””â”€â”€ js/
â”‚       â”œâ”€â”€ main.js     # ê²Œì„ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”‚       â””â”€â”€ game/       # ê·¸ë¦¬ë“œ, ì—ì´ì „íŠ¸, 15ê°œ RL ì•Œê³ ë¦¬ì¦˜ + DQN(ì‹¤í—˜), ë Œë”ëŸ¬, ì—ë””í„°, ì‚¬ìš´ë“œ, ì ˆì°¨ì  ìƒì„±ê¸°
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ dungeons/       # ë˜ì „ íŒŒì¼ë“¤ (3ê°œ, ë‚˜ë¨¸ì§€ 28ê°œëŠ” grid.js í•˜ë“œì½”ë”©)
â”œâ”€â”€ tests/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ GDD.md          # ê²Œì„ ë””ìì¸ ë¬¸ì„œ
â”‚   â””â”€â”€ TASK_BREAKDOWN.md  # ì„¸ë¶€ ì‘ì—… ê³„íš
â”œâ”€â”€ play_game.py        # Pygame ì§ì ‘ í”Œë ˆì´
â”œâ”€â”€ train_agent.py      # Pygame AI í•™ìŠµ
â””â”€â”€ run_viewer.py       # ë˜ì „ ë·°ì–´
```

## Q-Learning ê²°ê³¼ ì˜ˆì‹œ

```
=== level_01_easy.txt (5x5) ===
í•™ìŠµ: 500 ì—í”¼ì†Œë“œ
ì„±ê³µë¥ : 100%
í‰ê·  ìŠ¤í…: 4.0 (ìµœë‹¨ ê²½ë¡œ)

í•™ìŠµëœ ì •ì±…:
# # # # #
# v v v #
# v v v #
# > > G #
# # # # #
```

## ê¸°ìˆ  ìŠ¤íƒ

- **Python 3.10+**
- **Gymnasium**: RL í™˜ê²½
- **NumPy**: ìˆ˜ì¹˜ ì—°ì‚°
- **Pygame**: ê²Œì„ ë Œë”ë§
- **Matplotlib**: í•™ìŠµ ì‹œê°í™”

## ì£¼ìš” ê¸°ëŠ¥

- **31ê°œ ë˜ì „**: íŠœí† ë¦¬ì–¼ë¶€í„° ë…¼ë¬¸ ë²¤ì¹˜ë§ˆí¬, í™•ë¥ ì  ì „ì´ í™˜ê²½, 50Ã—50 ì ˆì°¨ì  ìƒì„± ë˜ì „ê¹Œì§€
- **ê³¨ë“œ ì´ì½”ë…¸ë¯¸**: ë˜ì „ ì…ì¥ë¹„, í´ë¦¬ì–´ ë³´ìƒ, ëª¬ìŠ¤í„° ì²˜ì¹˜ ë³´ìƒ
- **ë˜ì „ ì–¸ë½**: ì´ì „ ë˜ì „ í´ë¦¬ì–´ ì‹œ ë‹¤ìŒ ë˜ì „ í•´ê¸ˆ
- **ì „ì¥ì˜ ì•ˆê°œ**: ë°©ë¬¸í•œ ì¹¸ë§Œ ë³´ì´ëŠ” íƒí—˜ ì‹œìŠ¤í…œ
- **ëª¬ìŠ¤í„° ì‹œìŠ¤í…œ**: HP ë°ë¯¸ì§€ + ì²˜ì¹˜ ë³´ìƒ
- **HP-aware Q-Learning**: HP ìƒíƒœë¥¼ ê³ ë ¤í•œ í•™ìŠµ
- **Learning from Demonstration**: ìœ ì € í”Œë ˆì´ë¡œ AI í•™ìŠµ ê°€ì†
- **Q-Table ì €ì¥/ë³µì›**: localStorageë¡œ í•™ìŠµ ë°ì´í„° ë³´ì¡´
- **ëª¨ë°”ì¼ í„°ì¹˜ ì»¨íŠ¸ë¡¤**: ìŠ¤ì™€ì´í”„ + D-pad
- **AI í•™ìŠµ ì‹œê°í™”**: 4ë‹¨ê³„ ì†ë„ë¡œ í•™ìŠµ ê³¼ì • ì‹¤ì‹œê°„ ê´€ì°°
- **8ë¹„íŠ¸ ì‚¬ìš´ë“œ**: Web Audio API ê¸°ë°˜ íš¨ê³¼ìŒ
- **15ê°œ RL ì•Œê³ ë¦¬ì¦˜**: Q-Learning, SARSA, Monte Carlo, SARSA(Î»), Dyna-Q, REINFORCE, Actor-Critic, Local Q-Learning, QV-Learning, ACLA, Ensemble, Expected SARSA, Double Q-Learning, n-step Tree Backup, Prioritized Sweeping
- **í™•ë¥ ì  ì „ì´ í™˜ê²½**: FrozenLake ìŠ¤íƒ€ì¼ ë¯¸ë„ëŸ¬ìš´ ë°”ë‹¥ (Slippery) ì§€ì›
- **ì•™ìƒë¸” ì‹œìŠ¤í…œ**: Boltzmann Multiplicationìœ¼ë¡œ 5ê°œ ì•Œê³ ë¦¬ì¦˜ ê²°í•© (Wiering & van Hasselt, 2008)
- **ë©€í‹°ìŠ¤í…Œì´ì§€ ë˜ì „**: ì—¬ëŸ¬ ì¸µì„ ë¬¶ì–´ í•˜ë‚˜ì˜ ë˜ì „ìœ¼ë¡œ êµ¬ì„±, HP ê³„ìŠ¹, ê³¨ë“œ ë³´ë¥˜
- **ë˜ì „ ì—ë””í„°**: ë¸Œë¼ìš°ì € ë‚´ íƒ€ì¼ ë°°ì¹˜, BFS ê²€ì¦, ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°, ì»¤ìŠ¤í…€ ë˜ì „ AI í›ˆë ¨
- **ì ˆì°¨ì  ë˜ì „ ìƒì„±**: BSP + Cellular Automata í•˜ì´ë¸Œë¦¬ë“œë¡œ 50Ã—50 ëŒ€ê·œëª¨ ë˜ì „ ìƒì„±

## í–¥í›„ ê³„íš

1. **NPC ê°€ì°¨**: ì•Œê³ ë¦¬ì¦˜ ìºë¦­í„°í™” (í”¼í”¼ì˜¤/PPO, ì‚­/SAC ë“±)
2. **Neural ì•Œê³ ë¦¬ì¦˜**: DQN ì¬í™œì„±í™” (50Ã—50+), PPO, A2C
3. **ë™ì  í™˜ê²½**: ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” ì¥ì• ë¬¼, ê¸°ì–µ ì˜ì¡´ ê³¼ì œ

## ì°¸ê³  ìë£Œ / ë…¼ë¬¸ ì¶œì²˜

**ê°•ì˜ ìë£Œ**
- [DeepMind RL Course (David Silver)](https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver) â€” Q-Learning, SARSA, MC, SARSA(Î»), Dyna-Q, REINFORCE, Actor-Critic
- [Hugging Face Deep RL Course](https://huggingface.co/learn/deep-rl-course/unit0/introduction)
- [Gymnasium Documentation](https://gymnasium.farama.org/)
- [Sutton & Barto (2018) "Reinforcement Learning: An Introduction" 2nd ed.](http://incompleteideas.net/book/the-book-2nd.html)

**ë…¼ë¬¸ â€” í˜„ì¬ êµ¬í˜„**
- Watkins & Dayan (1992) "Q-learning", Machine Learning â€” Q-Learning
- Rummery & Niranjan (1994) "On-line Q-learning using connectionist systems" â€” SARSA
- Sutton (1991) "Dyna, an integrated architecture for learning, planning, and reacting" â€” Dyna-Q
- Williams (1992) "Simple statistical gradient-following algorithms for connectionist RL" â€” REINFORCE
- Barto, Sutton & Anderson (1983) "Neuronlike adaptive elements..." â€” Actor-Critic
- Wiering & van Hasselt (2008) "Ensemble Algorithms in Reinforcement Learning", IEEE TSMCB â€” QV-Learning, ACLA, Ensemble (Boltzmann Multiplication)
- Mnih et al. (2015) "Human-level control through deep reinforcement learning", Nature 518 â€” DQN (ì‹¤í—˜ì )
- Farama Foundation, MiniGrid â€” ë¡œì»¬ ê´€ì¸¡(egocentric partial observation) ìƒíƒœ ì¸ì½”ë”© ì°¸ì¡°

**ë…¼ë¬¸ â€” Phase 14~15 ì¶”ê°€ ì•Œê³ ë¦¬ì¦˜**
- van Seijen et al. (2009) "A Theoretical and Empirical Analysis of Expected Sarsa" â€” Expected SARSA
- van Hasselt (2010) "Double Q-learning", NeurIPS â€” Double Q-Learning
- Moore & Atkeson (1993) "Prioritized Sweeping", Machine Learning â€” Prioritized Sweeping
- Sutton & Barto (2018) Section 7.5 "A Unifying Algorithm: n-step Tree Backup" â€” n-step Tree Backup

**ë²¤ì¹˜ë§ˆí‚¹ ì—°êµ¬**
- "Benchmarking Tabular RL Algorithms" (TDS, 2025) â€” 25Ã—25ê¹Œì§€ ì²´ê³„ì  ë¹„êµ
- "Revisiting Benchmarking of Tabular RL Methods" (TDS) â€” n-step Tree Backup ìµœìš°ìˆ˜

**ì ˆì°¨ì  ìƒì„±**
- BSP Tree + Cellular Automata í•˜ì´ë¸Œë¦¬ë“œ â€” 50Ã—50 ë˜ì „ ìë™ ìƒì„±

**í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ í™˜ê²½**
- Gymnasium: CliffWalking, WindyGridworld, FrozenLake
- MiniGrid (Farama): Empty, FourRooms, DoorKey, LavaGap, MultiRoom
- AI Safety Gridworlds (DeepMind, 2017)

## ë¼ì´ì„ ìŠ¤

MIT License
